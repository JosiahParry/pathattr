---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# Making R 200x + faster with Rust

<!-- badges: start -->
<!-- badges: end -->

The goal of pathattr is to demonstrate how extendr can be used in an R project to speed up processing time and memory efficiency.

Sometime we can only get so far with R only code. 

## Installation

You can install the development version of pathattr like so:

``` r
remotes::install_github("josiahparry/pathattr")
```

## Problem

Let's look at the problem that we are trying to solve first. The package includes some sample data that we can work with. 

```{r example}
library(pathattr)

head(path10k)
```

The desired output for a single row looks like: 

```
#>    channel_name   re conversion     value      dates
#> 1:       tiktok 0.10 0.09615385  653.2879 2023-04-17
#> 2:         blog 0.09 0.08653846  587.9591 2023-02-25
#> 3:           gs 0.60 0.57692308 3919.7273 2023-01-24
#> 4:           fb 0.20 0.19230769 1306.5758 2023-03-12
#> 5:          rtl 0.05 0.04807692  326.6439 2023-03-09
```

The values of re are derived from a lookup table. 

```{r}
lu
```

Conversion rates and lead value are derived from these (I think).

## Bench mark:

```{r}
path_data <- path10k[1:1000,]

bm <- bench::mark(
  # original = data.table::rbindlist(
  #   purrr::pmap(
  #     list(path_str=path_data$path,date_str=path_data$dates,
  #          outcome=path_data$leads,value=path_data$value),
  #     attribute_path,
  #     removal_effects_table,
  #     .progress = 'path_level'
  #   )
  # ),
  # mine = data.table::rbindlist(
  #   purrr::pmap(
  #     list(path_str=path_data$path,date_str=path_data$dates,
  #          outcome=path_data$leads,value=path_data$value),
  #     attr_path2,
  #     lu,
  #     .progress = 'path2_level'
  #   )
  # ),
  rust = data.table::rbindlist(attr_path(
    path_data$path,
    path_data$dates,
    path_data$leads,
    path_data$value,
    as.list(lu)
  )),
  data.table = {
    touches <- strsplit(path_data$path, ">", fixed = TRUE)
    lt <- lengths(touches)
    groups <- rep.int(seq_along(touches), lt)
    outcome <- rep.int(path_data$leads, lt)
    value <- rep.int(path_data$value, lt) 
    touches <- unlist(touches)
    dates <- unlist(strsplit(path_data$dates, ">", fixed = TRUE))
    not_empty <- touches != ''
    dates <- dates[not_empty] 
    touches <- touches[not_empty]
    re <- lu[touches]
    DT <- data.table(
        channel_name = touches,
        outcome = outcome,
        date = dates,
        re,
        value,
        groups
    )
    DT[, re_tot := sum(re, na.rm = TRUE), by = groups]
    DT[, `:=`(conversion = outcome * re / re_tot, value = value * re / re_tot)]
    DT[,.(channel_name, re, conversion, value, date)]
  },
  dplyr = {
    touches <- strsplit(path_data$path, ">", fixed = TRUE)
    lt <- lengths(touches)
    groups <- rep.int(seq_along(touches), lt)
    outcome <- rep.int(path_data$leads, lt)
    value <- rep.int(path_data$value, lt) 
    touches <- unlist(touches)
    dates <- unlist(strsplit(path_data$dates, ">", fixed = TRUE))
    not_empty <- touches != ''
    dates <- dates[not_empty] 
    touches <- touches[not_empty]
    re <- lu[touches]
    df <- data.frame(
        channel_name = touches,
        outcome = outcome,
        date = dates,
        re,
        value,
        groups
     )
    df |> 
      mutate(
        re_tot = sum(re, na.rm = TRUE),
        conversion = outcome * re / re_tot,
        value = value * re / re_tot,
        .by = "groups"
      ) |> 
      select(channel_name, re, conversion, value, date) |> 
      as.data.table()
  }
)


bmbm |> 
    dplyr::select(1:5) |> 
    dplyr::mutate(
        times_faster = dplyr::coalesce(
          as.double(dplyr::lag(median, 2) / median),
          as.double(dplyr::lag(median, 1) / median)
          )
    )
```


## Testing with 1 million rows

```{r}
path_data <- dplyr::sample_n(path10k, 1000000, replace = TRUE)

start <- Sys.time()
res <- data.table::rbindlist(attr_path(
    path_data$path,
    path_data$dates,
    path_data$leads,
    path_data$value,
    as.list(lu)
  ))

end <- Sys.time()
end - start
```

Using data.table and base R. 

```{r}
path_data <- dplyr::sample_n(path10k, 3000000, replace = TRUE)

start <- Sys.time()
 touches <- strsplit(path_data$path, ">", fixed = TRUE)
    lt <- lengths(touches)
    groups <- rep.int(seq_along(touches), lt)
    outcome <- rep.int(path_data$leads, lt)
    value <- rep.int(path_data$value, lt) 
    touches <- unlist(touches)
    dates <- unlist(strsplit(path_data$dates, ">", fixed = TRUE))
    not_empty <- touches != ''
    dates <- dates[not_empty] 
    touches <- touches[not_empty]
    re <- lu[touches]
    DT <- data.table(
        channel_name = touches,
        outcome = outcome,
        date = dates,
        re,
        value,
        groups
    )
    DT[, re_tot := sum(re, na.rm = TRUE), by = groups]
    DT[, `:=`(conversion = outcome * re / re_tot, value = value * re / re_tot)]
    DT[,.(channel_name, re, conversion, value, date)]
    
end <- Sys.time()
end - start
```

Why is this so damn fast? Well, it is because R is super fast. This code leverages R's strengths. Namely, vectorization. In the original R approach and the one that I wrote we fell into the 3rd circle of hell according to Patrick Burns. We failed to vectorize. 
